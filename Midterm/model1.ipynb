{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 (Random Forest Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `pandas`, `numpy`: data manipulation <br>\n",
    "* `matplotlib`, `seaborn`: visualizations <br>\n",
    "* `sklearn`: ML and deep learning frameworks <br>\n",
    "* `OS`: to download and upload files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import os \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Getting data from kaggle<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"sid321axn/beijing-multisite-airquality-data-set\"\n",
    "DOWNLOAD_DIR = os.path.join(os.getcwd(), 'data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.data_util import fetch_data\n",
    "\n",
    "fetch_data(DATASET_NAME, DOWNLOAD_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining all the dataset files into one file called `combined_data.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.path.join(os.getcwd(), 'data')\n",
    "print(os.getcwd())\n",
    "\n",
    "print(\"data_folder\", data_folder)\n",
    "all_files = [os.path.join(data_folder, f) for f in os.listdir(data_folder) if f.endswith('.csv')]\n",
    "\n",
    "combined_data = pd.concat((pd.read_csv(file) for file in all_files), ignore_index=True)\n",
    "\n",
    "\n",
    "print(\"shape\" ,combined_data.shape)\n",
    "\n",
    "combined_data.to_csv(os.path.join(data_folder, 'combined_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/combined_data.csv')\n",
    "print(\"Shape:\", data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are missing values and random forest does not work if there are missing values, so we will find the missing values and fill them using forward_fill and backward_fill. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = data.isnull().sum()\n",
    "print(missing[missing > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.heatmap(data.isnull(), cbar=False, cmap='viridis')\n",
    "plt.title('Missing Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_percent = (data.isnull().sum() / len(data)) * 100\n",
    "missing_values = (missing_percent[missing_percent > 0].sort_values(ascending=False))\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(method='ffill')\n",
    "data = data.fillna(method='bfill')\n",
    "\n",
    "print(\"Values that are still missing\", data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.heatmap(data.isnull(), cbar=False, cmap='viridis')\n",
    "plt.title('Missing Values after filling')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature reduction\n",
    "### Removing `No` and `station` as it does not affect AQI. <br>\n",
    "### **Confirmed through trial and error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features = data.drop(columns=['No','station'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping PM2.5 as that we will use our output label. Also removing PM10 as its closely linked to PM2.5 meaning its indicator of AQI, and we dont our model to learn from nearly identical variable that is closely linked to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_features.drop(columns=['PM2.5','PM10'])\n",
    "y = data_features['PM2.5']\n",
    "\n",
    "print(\"X_shape\", X.shape)\n",
    "print(\"Y_shape\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.hist(y, bins=30)\n",
    "plt.title(\"PM2.5 Distribution\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will normalize the labels as models do better on normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log1p(y)  # Apply log transformation\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(y, bins=30)\n",
    "plt.title(\"Log of PM2.5 valuess\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.boxplot(y.dropna())  \n",
    "plt.title(\"Box Plot of PM2.5\")\n",
    "plt.ylabel(\"PM2.5 Values \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "for column in X.columns:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(X[column], kde=True, bins=30)\n",
    "    plt.title(f\"Distribution of {column}\")\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same here again we nomralizing the features as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['RAIN', 'SO2', 'NO2', 'CO', 'O3']\n",
    "for feature in features:\n",
    "    X[feature] = np.log1p(X[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in X.columns:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(X[column], kde=True, bins=30)\n",
    "    plt.title(f\"Distribution of {column}\")\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We converting wind direction to one hot encoding, as random forest cant take strings, so we can convert the directions into integers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded = pd.get_dummies(X, columns=['wd'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We made the correlation matrix to see how correlated the features are, we dont have that many features and also they dont correlate that much, so we dont have to do feature reduction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corr_matrix = X_encoded.corr()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### splitting the data based on years, into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = X_encoded[(X_encoded['year'] >= 2013) & (X_encoded['year'] <= 2016)]\n",
    "test = X_encoded[(X_encoded['year'] == 2017) & (X_encoded['month'] <= 2)]\n",
    "\n",
    "X_train = train\n",
    "y_train = y.loc[train.index]\n",
    "X_test = test\n",
    "y_test = y.loc[test.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the input as models train faster and with less bias on normalized input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print( X_train_scaled[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_scaled.shape)\n",
    "print(X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the model, and training, making predictions, and getting mean_absolute_error, mean_squared_error, r2_score\n",
    "### for the hyperparamters, down in the code we used **Random Search CV** to get the best hyperparameters and reusing it here, as random search CV will take 20+ mins to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=200,     \n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=1,\n",
    "    max_features='log2',\n",
    "    max_depth=None,   \n",
    "    random_state=25,       \n",
    "    n_jobs=-1,              \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R² Score: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Acutual vs predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test.values[:200], label='Actual')\n",
    "plt.plot(y_pred[:200], label='Predicted')\n",
    "plt.title(\"Actual vs Predicted (PM2.5)\")\n",
    "plt.ylabel(\"PM2.5 values\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to figure out which features are most important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf_model.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "feat_imp_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feat_imp_df)\n",
    "plt.title(\"Random Forest Feature Importances\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing those features that are not important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_importance_features = [\n",
    "    'wd_NNW',\n",
    "    'day',\n",
    "    'wd_SW',\n",
    "    'wd_WNW',\n",
    "    'RAIN',\n",
    "    'wd_SSW',\n",
    "    'wd_ESE',\n",
    "    'wd_SE',\n",
    "    'wd_SSE',\n",
    "    'wd_N',\n",
    "    'wd_S',\n",
    "    'wd_NNE',\n",
    "    'wd_WSW',\n",
    "    'wd_NE',\n",
    "    'wd_ENE',\n",
    "    'wd_W',\n",
    "    'wd_NW',\n",
    "]\n",
    "\n",
    "X_encoded_reduced = X_encoded.drop(columns=low_importance_features)\n",
    "train = X_encoded_reduced[(X_encoded_reduced['year'] >= 2013) & (X_encoded_reduced['year'] <= 2016)]\n",
    "test = X_encoded_reduced[(X_encoded_reduced['year'] == 2017) & (X_encoded_reduced['month'] <= 2)]\n",
    "X_train = train\n",
    "y_train = y.loc[train.index]\n",
    "X_test = test\n",
    "y_test = y.loc[test.index]\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R² Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test.values[:200], label='Actual', alpha=0.7)\n",
    "plt.plot(y_pred[:200], label='Predicted', alpha=0.7)\n",
    "plt.title(\"Actual vs Predicted (PM2.5)\")\n",
    "plt.xlabel(\"Time (hours)\")\n",
    "plt.ylabel(\"PM2.5 values\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf_model.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "feat_imp_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feat_imp_df)\n",
    "plt.title(\"Random Forest Feature Importances\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing if we can remove more redundant features while keeping the accuracy at same level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(columns=['year', 'hour', 'month','TEMP','PRES','WSPM'])\n",
    "test = test.drop(columns=['year', 'hour', 'month','TEMP','PRES','WSPM'])\n",
    "X_train = train\n",
    "y_train = y.loc[train.index]\n",
    "X_test = test\n",
    "y_test = y.loc[test.index]\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R² Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test.values[:200], label='Actual', alpha=0.7)\n",
    "plt.plot(y_pred[:200], label='Predicted', alpha=0.7)\n",
    "plt.title(\"Actual vs Predicted (PM2.5)\")\n",
    "plt.xlabel(\"Time (hours)\")\n",
    "plt.ylabel(\"PM2.5 Values\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf_model.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "feat_imp_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feat_imp_df)\n",
    "plt.title(\"Random Forest Feature Importances\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Random Search CV to try to get the best hyperparameters. This part takes very long, so we have commented it, but it will take 30 mins approx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample = X_encoded.sample(frac=0.1, random_state=42)\n",
    "y_sample = y.loc[X_sample.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from joblib import parallel_backend\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf_model, param_distributions=param_grid, n_iter=5, cv=3, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "with parallel_backend('threading'):\n",
    "    # Perform the random search\n",
    "    random_search.fit(X_sample, y_sample)\n",
    "print(\"Best Params\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with parallel_backend('threading'):\n",
    "    random_search.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Params\", random_search.best_params_)\n",
    "print(\"Best RMSE\", random_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = random_search.best_estimator_\n",
    "\n",
    "y_pred_tuned = best_rf.predict(X_test_scaled)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred_tuned)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_tuned))\n",
    "r2 = r2_score(y_test, y_pred_tuned)\n",
    "\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R²: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_rf.predict(X_test_scaled) \n",
    "residuals = y_test - y_pred\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(y_pred, residuals)\n",
    "plt.axhline(y=0, linestyle='--')\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.xlabel(\"Predicted PM2.5 Value\") \n",
    "plt.ylabel(\"Residual (Actual - Predicted)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = best_rf.feature_importances_\n",
    "feat_imp_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feat_imp_df)\n",
    "plt.title(\"Feature Importances of Random Search CV\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = best_rf.feature_importances_\n",
    "feature_names = X_train.columns if isinstance(X_train, pd.DataFrame) else range(X_train.shape[1])\n",
    "\n",
    "feat_imp = sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for feature, imp in feat_imp[:10]:  # top 10\n",
    "    print(feature, \":\", imp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph for predicted vs Acutual AQI values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.title(\"Predicted vs. Actual AQI Values\")\n",
    "plt.xlabel(\"Actual AQI Value\")\n",
    "plt.ylabel(\"Predicted AQI Value\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "### We reached an accuracy of 89%. Some features such as day, temperature and pressure did not contriubute much to AQI according to feature importance, so we removed that as they dont affect the acuracy much as well. As shown by the above graph, it shows a good accuracy, and only does worse on low AQI values, in which it highly overpredicts the value compared to actual. One explanation we think is that there might be not that much data on low AQI values or maybe the sensor readings at low level have high noise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
